{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Notebook to take the structured output from our OpenAI GPT-3 driver and convert it into the TSV format expected\n",
    "# by the MeasEval scoring script\n",
    "#\n",
    "# Note:  There are many cases where GPT-3 appears to have gotten into a loop repeating itself on output until\n",
    "# it hits the token length limit.  We'll have to run Corey's dedupe notebook to remove them in order to get a \n",
    "# like to like comparison with other MeasEval entries.\n",
    "\n",
    "# Note: GPT-3 at times will return lower-cased strings taht are equivilent to the paragraph, but at other times I've\n",
    "# seen it preserve case sensitivity. Right now I'm not accounting for that, although we could add additional logic\n",
    "# to check for both variants when determining offsets and whether or not to drop the annotation record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.3\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import platform\n",
    "print(platform.python_version())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity not in original para: between-group analyses\n",
      "WARNING: Entity not in original para: between-group analyses\n",
      "WARNING: Property not in original para: performances of the three classifiers\n",
      "WARNING: Property not in original para: performances of the three classifiers\n",
      "WARNING: Dropping a property because we don't have either/both of an associated Quantity or Entity\n",
      "WARNING: Dropping an Entity because we don't have an associated Quantity.\n",
      "WARNING: Quantity not in original para: 1016 cm−3\n",
      "WARNING: Unit not in original para: cm−3\n",
      "WARNING: Entity not in original para: injection level range\n",
      "WARNING: Quantity not in original para: 1013 cm−3\n",
      "WARNING: Unit not in original para: cm\n",
      "WARNING: Entity not in original para: monkey MHC-I allele\n",
      "WARNING: Dropping an Entity because we don't have an associated Quantity.\n",
      "Final droped annotation counts\n",
      "================================\n",
      "Number of Quantity annotations dropped: 2\n",
      "Number of Unit annotations dropped: 2\n",
      "Number of Property annotations dropped: 3\n",
      "Number of Entity annotations dropped: 6\n"
     ]
    }
   ],
   "source": [
    "numQuantityDropped = 0\n",
    "numUnitDropped = 0\n",
    "numPropertyDropped = 0\n",
    "numEntityDropped = 0\n",
    "\n",
    "\n",
    "workEntity = ''\n",
    "workEntityOffset = -1\n",
    "workUnit = ''\n",
    "workQuantity = ''\n",
    "workQuantityOffset = -1\n",
    "workProperty = ''\n",
    "workPropertyOffset = -1\n",
    "workAnnotSet = 1\n",
    "workAnnotId = 1\n",
    "\n",
    "# Output dubug when running\n",
    "DEBUG = False\n",
    "\n",
    "def resetWorkVars():\n",
    "    global workEntity, workEntityOffset, workUnit, workQuantity, workQuantityOffset, workProperty, workPropertyOffset\n",
    "    workEntity = ''\n",
    "    workEntityOffset = -1\n",
    "    workUnit = ''\n",
    "    workQuantity = ''\n",
    "    workQuantityOffset = -1\n",
    "    workProperty = ''\n",
    "    workPropertyOffset = -1\n",
    "\n",
    "# Convert our raw GPT-3 output records to TSV format. In our case, an annotation set (which is really what we are processing here)\n",
    "# should have a quantity along with an optional unit, property, and entity.\n",
    "def generateTsvAnnots():\n",
    "    global workEntity, workEntityOffset, workUnit, workQuantity, workQuantityOffset, workProperty, workPropertyOffset, workAnnotSet, workAnnotId, doc_id, numQuantityDropped, numUnitDropped, numPropertyDropped, numEntityDropped\n",
    "    workAnnots = []\n",
    "    workStr = ''\n",
    "    quantityId = -1\n",
    "    propertyId = -1\n",
    "    \n",
    "    QUANTITY_FMT = '''{}\\t{}\\tQuantity\\t{}\\t{}\\t{}\\t{}\\t'''\n",
    "    ENTITY_FMT = '''{}\\t{}\\tMeasuredEntity\\t{}\\t{}\\t{}\\t{}\\t{{\"{}\":\"{}\"}}'''\n",
    "    PROPERTY_FMT = '''{}\\t{}\\tMeasuredProperty\\t{}\\t{}\\t{}\\t{}\\t{{\"HasQuantity\":\"{}\"}}'''\n",
    "    \n",
    "    # strip off the string suffix to get just the docId \n",
    "    doc_id = doc_id.replace('.txt','')\n",
    "    \n",
    "    # Do we have a Quantity for this annotation set? Remember, we might have reset the Quantity to '' because it didn't\n",
    "    # exist in the actual paragraph being processed\n",
    "    if (workQuantity != ''):\n",
    "        workStr = QUANTITY_FMT.format(doc_id, workAnnotSet, workQuantityOffset, len(workQuantity) + workQuantityOffset, workAnnotId, workQuantity)\n",
    "        if  workUnit != '':\n",
    "            workStr = workStr + '{ \"unit\": \"' + workUnit + '\"}'\n",
    "        quantityId = workAnnotId\n",
    "        workAnnotId = workAnnotId+ 1\n",
    "        workAnnots.append(workStr)\n",
    "    if (workProperty != ''):\n",
    "        # MeasuredProperties require both a Measured Entity and Quantity.  Drop any Properties that GPT-3 might have \n",
    "        # found without those additional fields\n",
    "        if workQuantity != '':\n",
    "            workStr = PROPERTY_FMT.format(doc_id, workAnnotSet, workPropertyOffset, len(workProperty) + workPropertyOffset, workAnnotId, workProperty, quantityId, '\\n') \n",
    "            propertyId = workAnnotId\n",
    "            workAnnotId = workAnnotId+ 1\n",
    "            workAnnots.append(workStr)\n",
    "        else:\n",
    "            print(\"WARNING: Dropping a property because we don't have either/both of an associated Quantity or Entity\")\n",
    "            numPropertyDropped += 1\n",
    "            workProperty = ''\n",
    "    if workEntity != '':\n",
    "        # Entities require at least a Quantity to relate them to. There may also be an optional Property. Depending on\n",
    "        # what is present in the Annotation Set, we output it with the proper relationship mapping.\n",
    "        if workQuantity != '':\n",
    "            if workProperty != '':\n",
    "                workStr = ENTITY_FMT.format(doc_id, workAnnotSet, workEntityOffset, len(workEntity) + workEntityOffset, workAnnotId, workEntity, \"HasProperty\", propertyId) \n",
    "            else:\n",
    "                workStr =  ENTITY_FMT.format(doc_id, workAnnotSet, workEntityOffset, len(workEntity) + workEntityOffset, workAnnotId, workEntity, \"HasQuantity\", quantityId)\n",
    "            workAnnotId = workAnnotId+ 1\n",
    "            workAnnots.append(workStr)\n",
    "        else:\n",
    "            print(\"WARNING: Dropping an Entity because we don't have an associated Quantity.\")\n",
    "            numEntityDropped += 1\n",
    "    \n",
    "    resetWorkVars()\n",
    "    return workAnnots\n",
    "    \n",
    "\n",
    "# We've got 6 results files (since we broke the 135 MeasEval paragraphs into sets of 25 before before submitting them to GPT-3)\n",
    "resultFile = '/Users/kohlerc/Desktop/GPT-3/official_Run/tmpResults/keys_af.json'\n",
    "\n",
    "with open(resultFile) as f:\n",
    "  rawdata = f.read()\n",
    "  \n",
    "  # Note: we had to do some additional massaging of the results generated by our GPT-3 driver. Specifically, Python's\n",
    "  # JSON parsing didn't seem to like the embedded \\n characters even through the strings passed other JSON validators.\n",
    "  data = rawdata.replace('\\n', '\\\\n')\n",
    "  data = data.replace(',\\\\n', ',\\n')\n",
    "  data = data.replace('\\\\n\"\\\\n}', '\"}')\n",
    "  data = data.replace('\"\\\\n}', '\"}')\n",
    "  data = data.replace('}\\\\n]}\\\\n', '}]}')\n",
    "  \n",
    "  if DEBUG == True:\n",
    "    print(\"Massaged GPT-3 Results: \"  +  data)\n",
    "  \n",
    "  # Turn our string into a JSON Object\n",
    "  jsonData = json.loads(data)\n",
    "\n",
    "  \n",
    "  results = jsonData['results']\n",
    "\n",
    "  # Let's process each result in the JSON structure. Each should correspond to 1 paragraph we submitted\n",
    "  # and will have a docId, a finish code (\"stop\" or \"length\") from GPT-3, and a formatted \"text: \" section\n",
    "  # for the returned text from GPT-3. The text should consist of a formatted \"Data:\" section consisting of\n",
    "  # predicted \"Quantity\", \"Property\", and \"Entity\" where applicable.\n",
    "    \n",
    "  # Note: There was at least one response I saw where GPT-3 erroneously output some text before falling into \n",
    "  # the Data: pattern.\n",
    "  for result in results:\n",
    "    doc_id = result['doc']\n",
    "    finish_reason = result['finish_reason']\n",
    "    text = result['text']\n",
    "    # Every annotation in the TSV file has a unique annotation id. We reset the id at each new paragraph.\n",
    "    workAnnotId = 1\n",
    "    # All the realted annotations are in an unique annotation set for the paragraph. We reset that annotation set id\n",
    "    # for each paragraph\n",
    "    workAnnotSet = 1\n",
    "    outputStr = ''\n",
    "    outputAnnots = []\n",
    "    \n",
    "    if DEBUG == True:\n",
    "      print(\"########################################################\")\n",
    "      print(\"processing doc: \" + doc_id)\n",
    "      print(\"finish_reason: \" + finish_reason)\n",
    "    \n",
    "    # Read in the paragraph text that was submitted to OpenAI so we can calcualte the offsets needed for the TSV\n",
    "    with open ('/Users/kohlerc/github-2020/MeasEval/data/eval/text/' + doc_id) as para_file:\n",
    "        # Create the TSV file we're going to create for the raw results file\n",
    "        with open('/Users/kohlerc/Desktop/GPT-3/official_Run/tsv/' + doc_id.replace('.txt', '.tsv'), 'w') as tsv_file:\n",
    "            # Put out the TSV file header\n",
    "            tsv_file.write('{}\\n'.format('docId\\tannotSet\\tannotType\\tstartOffset\\tendOffset\\tannotId\\ttext\\tother'))\n",
    "            \n",
    "            para = para_file.read()\n",
    "            if DEBUG == True:\n",
    "                print('Original Para: ' + para)\n",
    "            # New paragraph so reset all the work variables    \n",
    "            resetWorkVars()\n",
    "            \n",
    "            # process the lines from the text predictions from GPT-3 and start buiding up the TSV records. Each line\n",
    "            # should be an annotation of some sort\n",
    "            predictionLines = text.strip().split('\\n')\n",
    "            for line in predictionLines:\n",
    "                if line.startswith(\"Data:\") == True:\n",
    "                    # Skip the \"Data:\"\" line\n",
    "                    x = 0\n",
    "                elif line.startswith('Quantity: '):\n",
    "                    workQuantity = line.split(\"Quantity: \",1)[1]\n",
    "                    workQuantityOffset = para.find(workQuantity)\n",
    "                    # GPT-3 sometimes goes off the reservation and returns text that isn't in the original paragraph\n",
    "                    # Somnetimes extra characters, sometimes completely different text. This is true for all 3 types\n",
    "                    # of annotations.\n",
    "                    # We'll drop the Quantity if that is the case:\n",
    "                    if workQuantityOffset == -1:\n",
    "                        print(\"WARNING: Quantity not in original para: \" + workQuantity)\n",
    "                        numQuantityDropped += 1\n",
    "                        workQuantity = ''\n",
    "\n",
    "                elif line.startswith('Unit: '):\n",
    "                    workUnit = line.split(\"Unit: \",1)[1]\n",
    "                    if para.find(workUnit) == -1:\n",
    "                        print(\"WARNING: Unit not in original para: \" + workUnit)\n",
    "                        numUnitDropped += 1\n",
    "                        workUnit = ''\n",
    "                elif line.startswith('Entity: '):\n",
    "                    workEntity = line.split(\"Entity: \",1)[1]\n",
    "                    workEntityOffset = para.find(workEntity)\n",
    "                    if workEntityOffset == -1:\n",
    "                        print(\"WARNING: Entity not in original para: \" + workEntity)\n",
    "                        numEntityDropped += 1\n",
    "                        workEntity = ''\n",
    "                elif line.startswith('Property: '):\n",
    "                    workProperty = line.split(\"Property: \",1)[1]\n",
    "                    workPropertyOffset = para.find(workProperty)\n",
    "                    if workPropertyOffset == -1:\n",
    "                        print(\"WARNING: Property not in original para: \" + workProperty)\n",
    "                        numPropertyDropped += 1\n",
    "                        workProperty = ''\n",
    "                # If we've moved onto a new annotation set for the same paragraph\n",
    "                # format the data for the TSV file and save it off for writing out later.\n",
    "                elif line.startswith(''):\n",
    "                    outputAnnots.extend(generateTsvAnnots())\n",
    "                    workAnnotSet = workAnnotSet + 1\n",
    "                else:\n",
    "                    print(\"unknown line type:'\" + line +\"'\")\n",
    "            # Pick up the last annot in the file to convert (if there is one)\n",
    "            if (workQuantity != ''):\n",
    "                outputAnnots.extend(generateTsvAnnots())\n",
    "            \n",
    "            if DEBUG == True:\n",
    "              print('writing out TSV file to disk.')\n",
    "                \n",
    "            for annot in outputAnnots:\n",
    "                if DEBUG == True:\n",
    "                    print(annot)\n",
    "                tsv_file.write('{}\\n'.format(annot))\n",
    "        \n",
    "            \n",
    "  print(\"Final droped annotation counts\")\n",
    "  print('================================')\n",
    "  print('''Number of Quantity annotations dropped: {}'''.format(numQuantityDropped))\n",
    "  print('''Number of Unit annotations dropped: {}'''.format(numUnitDropped))\n",
    "  print('''Number of Property annotations dropped: {}'''.format(numPropertyDropped))\n",
    "  print('''Number of Entity annotations dropped: {}'''.format(numEntityDropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ml)",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
